{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05526988a9a559e57a37a26d6d98a1a9",
     "grade": false,
     "grade_id": "cell-40f64c0268613244",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 5: Mining Sequence Data\n",
    "\n",
    "In this assignment, we will explore the sequence representation of data. Lots of real-world data can be represented as sequences. Among them, text data is typical and widely available, which we will use for this assignment. We will look at the Tweets with the colorful emojis again, but this time we are not going to filter the  textual content. Several toolkits or packages have been developed to process text data. In this assignment, we will rely on the [NLTK](https://www.nltk.org/) package (Natual Language Toolkit).\n",
    "\n",
    "In this assignment, you will: \n",
    "* Tokenize text data and extract ngrams and skip-grams.\n",
    "* Finding near-duplicate sequences of a given piece of text.  \n",
    "\n",
    "First, let's load the dependencies and the Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "024c36dc5f933ad0e192be4f64756758",
     "grade": false,
     "grade_id": "cell-3883ecbd0f3cfad0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import nltk\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "823ad79d7ca888f0fca14a06a089d470",
     "grade": false,
     "grade_id": "cell-ad2e286473e795bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"RT @stunnaboyco: @LUSCIOUSLOUIS that's lunchtime ğŸ©ğŸ«ğŸ«ğŸ¨ rite there ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ˜ŠğŸ˜ŠğŸ˜ğŸ˜ğŸ˜— https://t.co/m0Wngznctw\",\n",
       " '@RossoRestaurant looking forward to joining you for lunch tomorrow to celebrate my husbands birthday! ğŸ‚ğŸ¥‚',\n",
       " 'Out wth my lilbro 4 ice-creamğŸ¡ğŸ¦ğŸ¦ğŸ¦ https://t.co/dFEimQiF3n',\n",
       " 'KukuğŸ‘ le DicksonğŸ† tsa ba bangwe di behaviour okare Clientelle funeral cover They can cover up to 15 guys/ladies. #TheLivingSoulğŸ‘´',\n",
       " 'RT @Thabang92416252: \"@PinexAndApplex: ğŸğŸ @EmteeSA - We upğŸ”¥ https://t.co/nGbvFW2CUA\"YEE']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = []\n",
    "with open('more_tweets.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if len(line) > 0:\n",
    "            tweets.append(line.strip())\n",
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fea13200b5a091da0379f7c05255780f",
     "grade": false,
     "grade_id": "cell-06949c03757d6cae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To construct the sequence representation of the Tweet, we need to tokenize each Tweet into a sequence of language units, which in our case are words. For this assignment, we will use the TweetTokenizer API. For some languages, however, tokenization can be challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a763c758b059f261060b28e4f4d5d5f",
     "grade": false,
     "grade_id": "cell-85f395369e713527",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.casual.TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe34b0d4ea4828ca2dc87f5ed878c4a0",
     "grade": false,
     "grade_id": "cell-2ca6e08c67060260",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " '@stunnaboyco',\n",
       " ':',\n",
       " '@LUSCIOUSLOUIS',\n",
       " \"that's\",\n",
       " 'lunchtime',\n",
       " 'ğŸ©',\n",
       " 'ğŸ«',\n",
       " 'ğŸ«',\n",
       " 'ğŸ¨',\n",
       " 'rite',\n",
       " 'there',\n",
       " 'ğŸ‘',\n",
       " 'ğŸ‘',\n",
       " 'ğŸ‘',\n",
       " 'ğŸ‘',\n",
       " 'ğŸ˜Š',\n",
       " 'ğŸ˜Š',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ˜—',\n",
       " 'https://t.co/m0Wngznctw']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0152bce7d0307019c9a7a4686db34352",
     "grade": false,
     "grade_id": "cell-634c84b8ac40331b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For a quick sanity check, we can calculate the word frequency and see which ones are the most frequently used. With the Counter object, we can easily obtain the most frequently used words and their numbers of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "940600a9af2e4b7ac48f8425c263b7ea",
     "grade": false,
     "grade_id": "cell-6e7d9a6906d635db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('!', 4748),\n",
       " (':', 4601),\n",
       " ('RT', 3759),\n",
       " ('ï¸', 3011),\n",
       " ('â€¦', 2711),\n",
       " ('.', 2409),\n",
       " ('ğŸ‚', 2135),\n",
       " (',', 2083),\n",
       " ('a', 1830),\n",
       " ('to', 1819),\n",
       " ('the', 1806),\n",
       " ('you', 1670),\n",
       " ('and', 1647),\n",
       " ('ğŸ¾', 1413),\n",
       " ('I', 1404),\n",
       " ('ğŸ‰', 1294),\n",
       " ('ğŸ¥‚', 1274),\n",
       " ('Happy', 1270),\n",
       " ('ğŸ°', 1226),\n",
       " ('ğŸ»', 1183)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_counter = Counter()\n",
    "for tweet in tweets:\n",
    "    unigram_list = tokenizer.tokenize(tweet)\n",
    "    unigram_counter.update(unigram_list)\n",
    "unigram_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bf03d6d9f184e705a3fdfa756c1f8b7",
     "grade": false,
     "grade_id": "cell-61814a39523457fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "One common type of defined sequential patterns are $n$-grams. Particularly, 1-grams are often called \"unigrams\", 2-grams called bigrams, and 3-grams trigrams. Let's examine the bigrams of a Tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe30acd62628c484dddcb6c159d43ee7",
     "grade": false,
     "grade_id": "cell-9a63cf1c82603cf8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RT', '@stunnaboyco'),\n",
       " ('@stunnaboyco', ':'),\n",
       " (':', '@LUSCIOUSLOUIS'),\n",
       " ('@LUSCIOUSLOUIS', \"that's\"),\n",
       " (\"that's\", 'lunchtime'),\n",
       " ('lunchtime', 'ğŸ©'),\n",
       " ('ğŸ©', 'ğŸ«'),\n",
       " ('ğŸ«', 'ğŸ«'),\n",
       " ('ğŸ«', 'ğŸ¨'),\n",
       " ('ğŸ¨', 'rite'),\n",
       " ('rite', 'there'),\n",
       " ('there', 'ğŸ‘'),\n",
       " ('ğŸ‘', 'ğŸ‘'),\n",
       " ('ğŸ‘', 'ğŸ‘'),\n",
       " ('ğŸ‘', 'ğŸ‘'),\n",
       " ('ğŸ‘', 'ğŸ˜Š'),\n",
       " ('ğŸ˜Š', 'ğŸ˜Š'),\n",
       " ('ğŸ˜Š', 'ğŸ˜'),\n",
       " ('ğŸ˜', 'ğŸ˜'),\n",
       " ('ğŸ˜', 'ğŸ˜—'),\n",
       " ('ğŸ˜—', 'https://t.co/m0Wngznctw')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_list = list(nltk.bigrams(tokenizer.tokenize(tweets[0])))\n",
    "bigram_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fab7995160ec4d261a72d0c4591b227c",
     "grade": false,
     "grade_id": "cell-a358a9406182fc78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note that each bigram is represented as a tuple of two strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10828d8ef62c19a46044d54b3ec86b70",
     "grade": false,
     "grade_id": "cell-7e3d9f67e0213759",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1. Find the most frequent bigrams (1.5 pts)\n",
    "\n",
    "Please complete the `freq_bigram` function to find the $n$ most frequent bigrams. Your function should return a list of `top_n` tuples, each of the tuples should contain a bigram tuple (such as ('ğŸ‘', 'ğŸ‘')) and its number of occurrence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e95e4b9e609f1fd83ff13b711580cb89",
     "grade": false,
     "grade_id": "cell-c1116778c8dfe183",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def freq_bigrams(tweets, top_n):\n",
    "    bigram_counter = Counter()\n",
    "    for tweet in tweets:\n",
    "        # YOUR CODE HERE\n",
    "        bigram_list = list(nltk.bigrams(tokenizer.tokenize(tweet)))\n",
    "        bigram_counter.update(bigram_list)\n",
    "    return bigram_counter.most_common(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('!', '!'), 1334),\n",
       " (('â¤', 'ï¸'), 624),\n",
       " (('Happy', 'Birthday'), 570),\n",
       " (('â€™', 's'), 483),\n",
       " (('â˜•', 'ï¸'), 450),\n",
       " (('Happy', 'birthday'), 347),\n",
       " (('ğŸ¾', 'ğŸ¥‚'), 288),\n",
       " (('ğŸ‚', 'ğŸ‚'), 277),\n",
       " (('ğŸ‚', 'ğŸ°'), 276),\n",
       " (('â˜€', 'ï¸'), 274)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_bigrams(tweets, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5dcee4564f2ceb00a8a1f4684dc955c",
     "grade": true,
     "grade_id": "cell-8aaf916693fdac5c",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test\n",
    "# This test cell contains hidden tests. Passing the displayed assertions does not guarant full points.\n",
    "answer = freq_bigrams(tweets, 10)\n",
    "assert answer[0] == (('!', '!'), 1334)\n",
    "\n",
    "answer2 = freq_bigrams(tweets, 6)\n",
    "assert len(answer2) == 6\n",
    "assert answer2[5] == (('Happy', 'birthday'), 347)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8265ded396050c66ded5fdd83d71287",
     "grade": false,
     "grade_id": "cell-9e105411817a7835",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Similarly, you can generate trigrams by calling the `nltk.trigrams` API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cba2d268d95ce41937c573dc29df128",
     "grade": false,
     "grade_id": "cell-7970a41d638fe9ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2. Find the most frequent skipgrams (1 pt)\n",
    "\n",
    "In this exercise we will compute another commonly defined type of sequential patterns -- the skip-grams. Luckily this is also supported by NLTK. You can find the documentation [here](https://tedboy.github.io/nlps/generated/generated/nltk.skipgrams.html).\n",
    "\n",
    "Please implement the `freq_skipgrams` function to calculate the most frequently used $k$-skip-$n$-grams. Your function should return a list of `top_n` tuples, each of the tuples should contain a $k$-skip-$n$-gram tuple (such as ('Happy', 'Birthday', 'ğŸ‚')) and its number of occurrences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2516dfc27709864d28ddbdf06ce5f123",
     "grade": false,
     "grade_id": "cell-dd5f5da07000fdc8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def freq_skipgrams(tweets, n, k, top_n):\n",
    "    skipgram_counter = Counter()\n",
    "    # YOUR CODE HERE\n",
    "    for tweet in tweets:\n",
    "        skipgram_list = list(nltk.skipgrams(tokenizer.tokenize(tweet), n, k))\n",
    "        skipgram_counter.update(skipgram_list)\n",
    "    return skipgram_counter.most_common(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('!', '!'), 1334),\n",
       " (('â¤', 'ï¸'), 624),\n",
       " (('Happy', 'Birthday'), 570),\n",
       " (('â€™', 's'), 483),\n",
       " (('â˜•', 'ï¸'), 450),\n",
       " (('Happy', 'birthday'), 347),\n",
       " (('ğŸ¾', 'ğŸ¥‚'), 288),\n",
       " (('ğŸ‚', 'ğŸ‚'), 277),\n",
       " (('ğŸ‚', 'ğŸ°'), 276),\n",
       " (('â˜€', 'ï¸'), 274)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_bigrams(tweets, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c0c850cc6078905062e9a20afd434cd",
     "grade": true,
     "grade_id": "cell-404194a7b8c6d451",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test\n",
    "answer = freq_skipgrams(tweets, n=3, k=2, top_n=10)\n",
    "assert answer[0] == (('!', '!', '!'), 511)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a87baccc4cb6b45ff1a2d1f4716957d",
     "grade": false,
     "grade_id": "cell-d9450f9b8cfa8258",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ngrams and skipgrams are commonly used in text mining, biological sequence mining, and behavior mining tasks. They are directly used as basis for tasks like phrase detection, named-entity detection, and motif detection, and they are also used as features for building machine learning models in general. Have fun using them in your own data analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "439ea221bf3ab70bb2a7e0e66023cab3",
     "grade": false,
     "grade_id": "cell-fa5dd9813b92f313",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Detecting Near Duplicates\n",
    "\n",
    "Sequence similarity metrics can be widely applied to many tasks. In this part of the assignment, let's look at an application of sequence similarity -- detecting near-duplicates.  Near-duplicate detection is commonly used in plagiarism detection, index selection for search engines, copy-paste detection, biological sequence alignments, and beyond. Following the introduction of *shingling* in the lecture, let's practice it on our Twitter dataset. The *shingling* approach relies on $n$-grams and Jaccard similarity. Luckily, we have already been familiar with both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f8d9a8f24d84bb00e52249e9f13fed9",
     "grade": false,
     "grade_id": "cell-5c8f8934c3eedddd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3. Shingling (1.5 pts)\n",
    "\n",
    "Complete the `shingling_jaccard_similarity` function to compute the similarity score between two pieces of text using the shingling approach. Specifically, you should (1) represent both text sequences as sets of overlapping $n$-grams ($n$ specified as an argument) and (2) compute the Jaccard similarity between the two sets. We have implemented a `jaccard_similarity` function for your convenience. \n",
    "\n",
    "**Hint**: \n",
    "1. You may use the `nltk.ngrams` API to obtain the $n$-grams. \n",
    "2. The `nltk.ngrams` API returns a iterator of tuples. you may wrap it up with `list()` to collect the $n$-grams as a list. You may checkout how we use `nltk.bigrams` in the beginning of this assignment as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89ff7b92b33dd9e77aa1f47073902b2b",
     "grade": false,
     "grade_id": "cell-6460a982dc84bc93",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(list_x, list_y):\n",
    "    set_x = set(list_x)\n",
    "    set_y = set(list_y)\n",
    "    intersection = set_x.intersection(set_y)\n",
    "    union = set_x.union(set_y)\n",
    "    return len(intersection) / len(union) if len(union) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edff56b6b09307e5754cf6495cfbdce0",
     "grade": false,
     "grade_id": "cell-0bcf1d6f6aed2733",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.casual.TweetTokenizer()\n",
    "def shingling_jaccard_similarity(text_x, text_y, n):\n",
    "    # YOUR CODE HERE\n",
    "    ngram_list1 = list(nltk.ngrams(tokenizer.tokenize(text_x), n))\n",
    "    ngram_list = list(nltk.ngrams(tokenizer.tokenize(text_y), n))\n",
    "    sim_score = jaccard_similarity(ngram_list1, ngram_list)\n",
    "    return sim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "x = \"to be or not to be\"\n",
    "y = \"not be or not to be\"\n",
    "z = \"be or not to not be\"\n",
    "\n",
    "print(shingling_jaccard_similarity(x,y, 3))\n",
    "print(shingling_jaccard_similarity(x,z, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56497ebb22a915ec79abf6b649c5eca4",
     "grade": true,
     "grade_id": "cell-5976f57480088624",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(shingling_jaccard_similarity(\"to be or not to be\", \"not be or not to be\", 3) - 0.6) < 1e-8\n",
    "assert abs(shingling_jaccard_similarity(\"to be or not to be\", \"be or not to not be\", 3) - 1/3) < 1e-8\n",
    "\n",
    "assert abs(shingling_jaccard_similarity(\"a b c a b\", \"a b c a b c a b c\", 3) - 1) < 1e-8\n",
    "assert abs(shingling_jaccard_similarity(\"a a a b b b c c c\", \"a b c a b c\", 2) - 1/3) < 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "331dc56d970d8183aea24de043ff7249",
     "grade": false,
     "grade_id": "cell-cf0eb9f87a032dc6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's see how this similarity function can help us detect near-duplicating tweets in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0785fd76de0d5ffb39b28d08706218cb",
     "grade": false,
     "grade_id": "cell-2c2ba5471c82ee4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tweet_df = pd.read_csv(\"more_tweets.txt\", sep=\"\\t\", header=None)\n",
    "tweet_df.columns = ['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a31e0e6132debe92868252788ee66cb",
     "grade": false,
     "grade_id": "cell-5003383e6882b1cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "One of the Tweet that caught our attention has the index of 220."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "022277e2595c38d2d6646384b977165b",
     "grade": false,
     "grade_id": "cell-61ed470985b51e32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­ @malangangels ğŸ­ ğŸ¸ Malang ğŸ¸ ğŸ’° dan â˜ by DM https://t.co/vS1Oukm3yq\n"
     ]
    }
   ],
   "source": [
    "print(tweet_df.loc[220].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c942c7ea3657f8a2eaf2f74180f7f304",
     "grade": false,
     "grade_id": "cell-53ad58fa0b0fdd7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's see whether we can find near duplicates of this Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69cc85c512ab546405b51e180fc187a9",
     "grade": false,
     "grade_id": "cell-a3ddad870d7a4a12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>#TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­ @malangangel...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>RT @dunk_sl: #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­...</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4906</th>\n",
       "      <td>RT @dunk_sl: #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­...</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>RT @dunk_sl: #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­...</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323</th>\n",
       "      <td>#TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­ @VaniaBDG_2 ...</td>\n",
       "      <td>0.391304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6109</th>\n",
       "      <td>#TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­ @Lanci_Mevit...</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>RT @dunk_sl: #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­...</td>\n",
       "      <td>0.346154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118</th>\n",
       "      <td>RT @dunk_sl: #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­...</td>\n",
       "      <td>0.346154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6575</th>\n",
       "      <td>Case of the Monday Blues? Nothing a KNOW Bette...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6577</th>\n",
       "      <td>RT @LunnMiss: #TwinPeaks ğŸŒ²ğŸ¦‰ğŸŒ² Â«Fire Walk With M...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text       sim\n",
       "220   #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­ @malangangel...  1.000000\n",
       "2342  RT @dunk_sl: #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­...  0.750000\n",
       "4906  RT @dunk_sl: #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­...  0.521739\n",
       "2280  RT @dunk_sl: #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­...  0.521739\n",
       "2323  #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­ @VaniaBDG_2 ...  0.391304\n",
       "6109  #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­ @Lanci_Mevit...  0.375000\n",
       "3651  RT @dunk_sl: #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­...  0.346154\n",
       "5118  RT @dunk_sl: #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­...  0.346154\n",
       "6575  Case of the Monday Blues? Nothing a KNOW Bette...  0.000000\n",
       "6577  RT @LunnMiss: #TwinPeaks ğŸŒ²ğŸ¦‰ğŸŒ² Â«Fire Walk With M...  0.000000"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df['sim'] = tweet_df.text.apply(lambda x: shingling_jaccard_similarity(tweet_df.loc[220].text, x, 3))\n",
    "tweet_df.sort_values('sim', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a57826ac1527eb1ce07a74d351c8176e",
     "grade": false,
     "grade_id": "cell-720ddb543dd9aef7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­ @malangangels ğŸ­ ğŸ¸ Malang ğŸ¸ ğŸ’° dan â˜ by DM https://t.co/vS1Oukm3yq\n",
      "RT @dunk_sl: #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­ @malangangels ğŸ­ ğŸ¸ Malang ğŸ¸ ğŸ’° dan â˜ by DM https://t.co/dV3ibBQhar\n",
      "RT @dunk_sl: #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­ @malangvhi ğŸ­ ğŸ¸ Malang ğŸ¸ ğŸ’° dan â˜ by DM https://t.co/tyqGu1id2S\n",
      "RT @dunk_sl: #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­ @KimiMlg ğŸ­ ğŸ¸ Malang ğŸ¸ ğŸ’° dan â˜ by DM https://t.co/1SIagPPTAP\n",
      "#TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­ @VaniaBDG_2 ğŸ­ ğŸ¸ Bandung ğŸ¸ ğŸ’° dan â˜ by DM https://t.co/82ZT2nv2Py\n",
      "#TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­ @Lanci_Mevita ğŸ­ ğŸ¸ Expo Cirebon ğŸ¸ ğŸ’° dan â˜ by DM https://t.co/KCyJE4cqH3\n",
      "RT @dunk_sl: #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­ @AmorPutri96 ğŸ­ ğŸ¸ Bandung ğŸ¸ ğŸ’° dan â˜ by DM https://t.co/GHNSo7Qc1U\n",
      "RT @dunk_sl: #TopListAngels ğŸ‘  Ready For BO ğŸ‘  ğŸ­ @AmorPutri96 ğŸ­ ğŸ¸ Bandung ğŸ¸ ğŸ’° dan â˜ by DM https://t.co/XeNo4Dal2A\n",
      "Case of the Monday Blues? Nothing a KNOW Better cookie &amp; cup of joe can't fix.ğŸª â˜•ï¸ #CookiesandCoffee #yummy\n",
      "RT @LunnMiss: #TwinPeaks ğŸŒ²ğŸ¦‰ğŸŒ² Â«Fire Walk With MeÂ» Ğrt.Dan May @Kyle_MacLachlan â˜•ï¸ğŸ¥§â˜•ï¸ @mfrost11ğŸ”¥ğŸ’”ğŸ”¥ @DAVID_LYNCH @LynchFoundation https://â€¦\n"
     ]
    }
   ],
   "source": [
    "near_dups = tweet_df.sort_values('sim', ascending=False).head(10)\n",
    "for x in list(near_dups.text):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dffc47f20e7d27530ad4d5c600970aaa",
     "grade": false,
     "grade_id": "cell-74a2bc16218167e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Indeed, we see that the top-8 Tweets look very similar! In fact, many of them are Retweets of the original Tweet, with only small variations on the user handler and/or URLs - so they are indeed near-duplicates. This concludes this assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
